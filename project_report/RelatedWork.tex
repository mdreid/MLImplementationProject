\section{Related Work}
Quinlan proposed C4.5 as an improvement to ID3 \cite{quinlan2014c4}. Later, Quinlan further improved C4.5 by changing how the algorithm handles continuous features \cite{quinlan1996improved}. Though we implement the original version of C4.5, this is relevant to our work (and a potential improvement to be made in future work) because we use principal component analysis (PCA) to perform dimensionality reduction.  While the original pixel values were integers in the range 0 to 255, PCA results in new features with floating-point values.

Much work has looked at ensemble methods, in which multiple learning algorithms are used to make predictions. We provide a brief review of previous work that has used $k$-NN, decision trees, or both in an ensemble method. Todorovski et al. trained several different classifiers on their training set and then used meta decision trees, which are decision trees that, instead of making a prediction at the leaf nodes, decide which base-level classifier to use to make the prediction \cite{todorovski2003combining}. They demonstrated that this stacking approach performed better than standard decision trees.

Fathi et al. combined $k$-nearest neighbor and decision tree in two ways \cite{FathiMazinani}. In Replacement Based Decisions, they trained a decision tree and then converted decisions into vectors of binary decision-features. These vectors were then used to train $k$-NN. In Adding Based Replacement, they added these vectors to the original feature set. They found that both variants improved performance on their intrusion detection system datasets.

Gimenez et al. combined logistic rogression and $k$-NN, using the weights from regression to do a weighted $k$-NN \cite{campillo2013improving}. They found improved performance of this ensemble approach over standard $k$-NN and standard logistic regression. 

Finally, Martinez-Otzeta et al. preprocessed their dataset with nearest neighbor, boosting their training set by duplicating incorrectly classified instances \cite{martinezk}.  Then they used ID3 to create a decision tree on this boosted dataset. They again found that this ensemble approach performed better than decision trees trained on the unboosted data. 