\section{Related Work}
Quinlan proposed C4.5 as an improvement to ID3 (cite original C4.5 work). Later, Quinlan further improved C4.5 by changing how the algorithm handles continuous features (cite - improved use of cts attributes). Though we implemented the original version of C4.5, this is relevant to our work (and a potential improvement to be made in future work) because we used Principal Components Analysis to perform dimensionality reduction.  While the original pixel values were integers in the range 0 to 255, PCA resulted in new features with floating-point values.
Quinlan proposed C4.5 as an improvement to ID3 \cite{quinlan2014c4}. Quinlan further improved C4.5 by changing how the algorithm handles continuous features (cite - improved use of cts attributes). This is relevant to our work because we treated pixel values as continuous numbers in the range of 0 to 255 (inclusive). 
The MNIST database of scanned images of handwritten digits is a well-known and frequently used dataset (cite - LeCun et al., 1998). It contains 60,000 examples for trainining and 10,000 examples for testing. Various machine learning algorithms have been used on this data, which makes it useful for benchmarking against. The algorithm that performs best on the dataset, as of the time of this writing, is a committee of 35 convolutional neural networks \cite{cirecsan2010deep}.
Much work has looked at ensemble methods, in which multiple learning algorithms are used to make predictions. 

We will now provide a brief review of previous work that has used $k$-NN, decision trees, or both in an ensemble method. Todorovski et al. trained several different classifiers on their training set and then used meta decision trees, which are decision trees that, instead of making a prediction at the leaf nodes, decide which base-level classifier to use to make the prediction \cite{todorovski2003combining}. They demonstrated that this stacking approach performs better than using standard decision trees.
Fathi et al. combine $k$-nearest neighbor and decision tree in two ways \cite{FathiMazinani}. In Replacement Based Decisions, they train a decision tree and then convert decisions into vectors of binary decision-features. These vectors are then used to train $k$-NN. In Adding Based Replacement, they add these vectors to the original feature set. They find that both variants improve performance on their intrusion detection system datasets.
Gimenez et al. combine logistic rogression and $k$-NN, using the weights from regression to do a weighted $k$-NN \cite{campillo2013improving}. They find improved performance of this ensemble approach over standard $k$-NN and standard logistic regression. Finally, Martinez-Otzeta et al. preprocess their dataset with nearest neighbor, boosting their training set by duplicating incorrectly classified instances and then use ID3 to create a decision tree \cite{martinezk}. They again find that this ensemble approach performs better than decision trees trained on the unboosted data. 

There has been much research into ensemble methods, in which multiple learning algorithms are combined in some fashion and used to make predictions. We will now provide a brief review of previous work that has used $k$-NN, decision trees, or both in an ensemble method.

 Todorovski et al. trained several different base-level classifiers on their training set and then used meta decision trees, which, instead of associating label predictions with the leaf nodes, decide which base-level classifier to use in order to make the prediction (cite Todorovski). They demonstrated that this stacking approach performs better than using standard decision trees.

Fathi et al. combine $k$-Nearest Neighbors and decision tree approaches in two ways (cite Fathi). In the first, called Replacement Based Decisions, they train a decision tree and then convert decisions into vectors of binary decision-features. These vectors are then used in $k$-Nearest Neighbors. In the second, called Adding Based Replacement, they add these vectors to the feature set as additional features. They find that both variants improve performance on their intrusion detection system datasets.

Gimenez et al. combine logistic regression and $k$-NN, using the weights from regression to determine the parameters of a weighted $k$-NN (cite Gimenez). They find that this ensemble approach yields improved performance when compared to just using standard $k$-NN or standard logistic regression. 

Finally, Martinez-Otzeta et al. preprocess their dataset with $k$-Nearest Neighbors, boosting their training set by duplicating  any incorrectly classified instances.  Next, they then use ID3 to create a decision tree (cite) using this boosted training set. They again find that this ensemble approach performs better than decision trees trained on the unboosted data. 

