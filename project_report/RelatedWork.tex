\section{Related Work}
Quinlan proposed C4.5 as an improvement to ID3 (cite original C4.5 work). Quinlan further improved C4.5 by changing how the algorithm handles continuous features (cite - improved use of cts attributes). This is relevant to our work because we treated pixel values as continuous numbers in the range of 0 to 255 (inclusive). 
The MNIST database of scanned images of handwritten digits is a well-known and frequently used dataset (cite - LeCun et al., 1998). It contains 60,000 examples for trainining and 10,000 examples for testing. Various machine learning algorithms have been used on this data, which makes it useful for benchmarking against. The algorithm that performs best on the dataset, as of the time of this writing, is a committee of 35 convolutional neural networks (cite Ciresan et al.)
Much work has looked at ensemble methods, in which multiple learning algorithms are used to make predictions. 

We will now provide a brief review of previous work that has used $k$-NN, decision trees, or both in an ensemble method. Todorovski et al. trained several different classifiers on their training set and then used meta decision trees, which are decision trees that, instead of making a prediction at the leaf nodes, decide which base-level classifier to use to make the prediction (cite Todorovski). They demonstrated that this stacking approach performs better than using standard decision trees.
Fathi et al. combine $k$-nearest neighbor and decision tree in two ways (cite Fathi). In Replacement Based Decisions, they train a decision tree and then convert decisions into vectors of binary decision-features. These vectors are then used to train $k$-NN. In Adding Based Replacement, they add these vectors to the original feature set. They find that both variants improve performance on their intrusion detection system datasets.
Gimenez et al. combine logistic rogression and $k$-NN, using the weights from regression to do a weighted $k$-NN (cite Gimenez). They find improved performance of this ensemble approach over standard $k$-NN and standard logistic regression. Finally, Martinez-Otzeta et al. preprocess their dataset with nearest neighbor, boosting their training set by duplicating incorrectly classified instances and then use ID3 to create a decision tree (cite). They again find that this ensemble approach performs better than decision trees trained on the unboosted data. 

