\section{Related Work}
Quinlan proposed C4.5 as an improvement to ID3 (cite original C4.5 work). Later, Quinlan further improved C4.5 by changing how the algorithm handles continuous features (cite - improved use of cts attributes). Though we implemented the original version of C4.5, this is relevant to our work (and a potential improvement to be made in future work) because we used Principal Components Analysis to perform dimensionality reduction.  While the original pixel values were integers in the range 0 to 255, PCA resulted in new features with floating-point values.

The MNIST database of scanned images of handwritten digits is a well-known and frequently used dataset (cite - LeCun et al., 1998). It contains 60,000 examples intended for training and another 10,000 examples for testing. Various machine learning algorithms have been used on this data, which makes it convenient for benchmarking and comparison. The algorithm that performs best on the dataset, as of the time of this writing, is a committee of 35 convolutional neural networks (cite Ciresan et al.)

There has been much research into ensemble methods, in which multiple learning algorithms are combined in some fashion and used to make predictions. We will now provide a brief review of previous work that has used $k$-NN, decision trees, or both in an ensemble method.

 Todorovski et al. trained several different base-level classifiers on their training set and then used meta decision trees, which, instead of associating label predictions with the leaf nodes, decide which base-level classifier to use in order to make the prediction (cite Todorovski). They demonstrated that this stacking approach performs better than using standard decision trees.

Fathi et al. combine $k$-Nearest Neighbors and decision tree approaches in two ways (cite Fathi). In the first, called Replacement Based Decisions, they train a decision tree and then convert decisions into vectors of binary decision-features. These vectors are then used in $k$-Nearest Neighbors. In the second, called Adding Based Replacement, they add these vectors to the feature set as additional features. They find that both variants improve performance on their intrusion detection system datasets.

Gimenez et al. combine logistic regression and $k$-NN, using the weights from regression to determine the parameters of a weighted $k$-NN (cite Gimenez). They find that this ensemble approach yields improved performance when compared to just using standard $k$-NN or standard logistic regression. 

Finally, Martinez-Otzeta et al. preprocess their dataset with $k$-Nearest Neighbors, boosting their training set by duplicating  any incorrectly classified instances.  Next, they then use ID3 to create a decision tree (cite) using this boosted training set. They again find that this ensemble approach performs better than decision trees trained on the unboosted data. 

