\section{Methods}
\subsection{Combining Approaches}
In our hybrid approach, we combine the C4.5 decision tree learning algorithm with a Nearest Neighbor algorithm. Figure \ref{fig:workflow} shows the control flow of operations on the data set used to produce the final classifier model.

After learning and pruning a decision tree using the C4.5 algorithm, for each leaf of the decision tree we associate a $k$-d tree.

\subsection{Dimensionality Reduction}
Since the number of features in MNIST dataset is relatively large (784 features), we initially experienced prohibitively long training times. In order to reduce the training time, we use the Python scikit-learn API to apply PCA to the training set to reduce the number of features to 100. Note that we use only the training and tuning data to identify the principal components and then apply the same transform to the testing data. This way we avoid any overfitting issues that might result from the inclusion of our testing data.

Next, this reduced-dimensionality training data is used to train a hybrid decision tree. After the pruning step, at each of the leaf nodes, we build a Nearest Neighbor model to make predictions instead of associating the usual plurality prediction. The following sections provide more details about our hybrid classifier's implementation.

\subsection{Decision Tree}
\input{DecisionTree}
\subsection{Nearest Neighbor}
The $k$-NN algorithm assumes all instances correspond to points in the $n$-dimensional space, where $n$ is the number of features. To classify an instance, $k$-NN finds the instance's $k$ closest neighbors and assigns it the label that is most common among those $k$ nearest neighbors. Multiple similarity/distance measures can be used to find the $k$ nearest neighbors to an instance, including the following: Euclidean, Hamming, or Manhattan. In this paper, we use the square of Euclidean distance as our distance measure for $k$-NN.

\subsubsection{Exhaustive Search}
Exhaustive Search is the naive version of $k$-NN. That is, we directly store all the training instances in the learning step. Then, to classify a new instance, we loop through all the training instances to find its $k$ nearest neighbors. Hence, the time complexity to find an instance's nearest neighbor using Exhaustive Search is $O(N)$. For this reason, the prediction time can be very long if the training set is large. This is why we also implement a $k$-d version of $k$-NN.

\subsubsection{$k$-d Tree}
The $k$-d tree version of $k$-NN stores the training instances in a $k$-d tree data structure during the training step, and then uses this tree to find the nearest neighbor to an instance during the predicting step. The time complexity to find an instance's nearest neighbor can be greatly improved by $k$-d tree to an average of $O(log_2N)$.

\subsection{Training and Testing}
We divide the MNIST dataset into two broad pools.  From the first pool we draw training and tuning sets.  From the second we draw the testing set used to assess the algorithm.

We test several settings of training and tuning sets.  The testing set always consists of 10,000 instances.  The training set sizes are 100, 500, 1000, 5000, and 10,000. For each setting, the tuning set is always 10\% the size of the training set.
