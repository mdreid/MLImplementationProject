\section{Methods}
	\subsection{Algorithms and Combining Methods}
		
	\subsection{Decision Tree}
	\subsection{Nearest Neighbor}
		$k$-\textit{NEAREST NEIGHBOR} ($k$-\textit{NN}) is an instance-based, or ``lazy'' learning method that stores the training examples in the learning phase, and estimates the target function for each new instance to be classified. This algorithm assumes all instances correspond to points in the $n$-dimensional space, and to classify an instance $i$, $k$-\textit{NN} finds the instance's $k$ closest neighbors and assigns \^{c} as $i$'s label, where \^{c} is the most common label among those $k$ nearest neighbors. The nearest neighbors can be found using one of these distance methods: Euclidean, Hamming, or Manhattan. In this paper, we use Euclidean as our distance method for $k$-\textit{NN}.
		\subsubsection{Exhaustive Search}
		Exhaustive Search is a straightforward way to implement $k$-\textit{NN}. That is, we store all the training instances in the learning step. To classify an instance, we loop through all the training instances to find its $k$ nearest neighbors. For this reason, the prediction time can be very long if the training set is large. This is why we also implement K-D tree in the next section to find an instance nearest neighbor
		
		\subsubsection{K-D Tree}
		The K-D tree version of $k$-\textit{NN} stores the training instances in a K-D data structure during the training step, and then uses this tree to find the nearest neighbor to an instance during the predicting step.
		

