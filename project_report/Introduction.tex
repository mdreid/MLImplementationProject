\section{Introduction}
Classification is the problem of identifying the category to which observations belong, using information encodes in features of the observation. There are a large number of algorithms that are used for classifying data, across a variety of domains.  We treat these algorithms as being divided into two broad groups: lazy learning algorithms and eager learning algorithms.

  Lazy learning is a form of supervised learning where - during the learning phase, the training examples are simply stored [].  Later, during testing, the testing examples are compared to these training examples using some sort of similarity measure.  There are several instance-based lazy learning algorithms. One of the best known is k-Nearest Neighbor (k-NN) []. During the learning phase, k-NN memorizes training examples. During the classification phase, k-NN uses a similarity measure (such as (squared) Euclidean distance or Hamming distance) to determine the k nearest training examples to a given testing example.  The predicted classification for the testing example is simply whichever label is held by a plurality of the k nearest training examples.  Unfortunately, while lazy learners are very fast during the learning phase, they are much slower during the classification phase, though approaches such as k-d trees sometimes ameliorate this disadvantage.
  
   In contrast with lazy learning we have the eager learning approach, a form of supervised learning in which there is a learning module, a classification module, and a model. Eager learning algorithms invest most of their effort in the learning phase rather than simply "memorizing" the training data like algorithms such as k-NN. Classification of new instances is usually done by the application of a set of rules encoded in the model. Decision tree learning is a widely used approach to this sort of learning. Standard decision trees work by encoding single-feature tests within the interior nodes of the tree and plurality-based predictions at the leaves [].  Decision tree structures are usually learned using information gain as a greedy heuristic [].
   
  Knowing that hybrid classification approaches often work well in practice [], we decided to investigate whether decision trees could be combined with the k-Nearest Neighbors approach.  Rather than working with the entire training set, we could generate k-NN implementations that use only a subset of training data, associated with a leaf node of the decision tree.

  In our hybrid approach, we learn a decision tree using the standard C4.5 algorithm and then associate with each leaf node a subset of the training data that passes the tests to reach that leaf node and a k-d implementation of k-NN learned on that subset. To assess the performance of our hybrid classifier relative to the standard decision tree, we downloaded the MNIST Handwritten Digit Dataset \cite{MNISTDatabase} and used it to generate training, tuning, and testing examples for our algorithm.
